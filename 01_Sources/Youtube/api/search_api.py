"""
search_api.py

YouTube Data API v3 ê¸°ë°˜ ê¸°ë³¸ ìˆ˜ì§‘ê¸°.
- í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰ (search.list)
- videoId ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ìƒì„¸ ì •ë³´(statistics í¬í•¨) ì¡°íšŒ (videos.list)
- raw/search/ ì•„ë˜ì— ë‚ ì§œ+í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ JSON ì €ì¥

í•„í„°(ì¹´í…Œê³ ë¦¬/IP ë“±)ëŠ” êµì°¨ê²€ì¦ ì „ì— ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•´
í•¨ìˆ˜ í‹€ë§Œ ë‚¨ê²¨ë‘ê³  ì‹¤ì œ í˜¸ì¶œì€ í•˜ì§€ ì•ŠëŠ”ë‹¤.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from youtube_client import YouTubeSearchClient, YouTubeStatsClient  # ğŸ”¹ ê³µí†µ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

# ------------------------------
# ì„¤ì •
# ------------------------------

HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]  # .../01_Sources/YouTube
RAW_DIR = PROJECT_ROOT / "raw" / "search"

# ------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------

LOG_DIR = PROJECT_ROOT / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / "youtube_search_api.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


# ------------------------------
# (ë¯¸ì‚¬ìš©) í•„í„° í•¨ìˆ˜ í‹€
# â†’ í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì‹¤ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
# ------------------------------

def _filter_by_category_example(items: List[Dict[str, Any]], allowed_categories: List[str]) -> List[Dict[str, Any]]:
    """
    ì´ í•¨ìˆ˜ëŠ” ì•„ì§ êµì°¨ê²€ì¦ ì „ì´ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    categoryId ê¸°ë°˜ í•„í„° ì˜ˆì‹œ êµ¬ì¡°ë§Œ ë‚¨ê²¨ë‘” ê²ƒ.
    """
    filtered: List[Dict[str, Any]] = []
    for item in items:
        snippet = item.get("snippet", {})
        category_id = snippet.get("categoryId") or item.get("categoryId")
        if category_id in allowed_categories:
            filtered.append(item)
    return filtered


# ------------------------------
# ê²€ìƒ‰ â†’ ìƒì„¸ì¡°íšŒ â†’ ì €ì¥ í”Œë¡œìš°
# ------------------------------

def search_and_collect(
    query: str,
    max_results: int = 50,
    order: str = "date",
    published_after: Optional[str] = None,
    region_code: Optional[str] = None,
    output_dir: Optional[Path] = None
) -> Path:
    """
    1. search.listë¡œ ì˜ìƒ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ
    2. videos.listë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    3. raw/search/ì— JSON ì €ì¥
    4. ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜

    í•„í„°(category/IP)ëŠ” í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    search_client = YouTubeSearchClient()
    stats_client = YouTubeStatsClient()

    logger.info("YouTube ê²€ìƒ‰ ì‹œì‘: query=%s, max_results=%d", query, max_results)

    # 1) ê²€ìƒ‰
    search_data = search_client.search(
        query=query,
        max_results=max_results,
        order=order,
        published_after=published_after,
        region_code=region_code,
    )

    search_items = search_data.get("items", [])
    video_ids: List[str] = [
        item["id"]["videoId"]
        for item in search_items
        if item.get("id", {}).get("kind") == "youtube#video"
    ]

    logger.info("ê²€ìƒ‰ ê²°ê³¼ ì˜ìƒ ìˆ˜: %d", len(video_ids))

    # 2) ìƒì„¸ ì¡°íšŒ
    details_data = stats_client.get_video_details(video_ids)
    detail_items = details_data.get("items", [])

    logger.info("ìƒì„¸ ì •ë³´ ìˆ˜ì‹  ì˜ìƒ ìˆ˜: %d", len(detail_items))

    # â€» ì—¬ê¸°ì—ì„œ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ í•„í„°ë¥¼ ë„£ì„ ìˆ˜ ìˆì§€ë§Œ,
    #    í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ìœ¼ë¡œë§Œ ë‚¨ê¸´ë‹¤.
    #
    # allowed_categories = ["1", "24", "31"]  # ì˜ˆ: Film & Animation, Entertainment, Anime/Animation ë“±
    # detail_items = _filter_by_category_example(detail_items, allowed_categories)

    # 3) ì €ì¥ ì¤€ë¹„
    if output_dir is None:
        output_dir = RAW_DIR
    output_dir.mkdir(parents=True, exist_ok=True)

    today_str = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    safe_query = "".join(c if c.isalnum() else "_" for c in query)[:50]
    filename = f"{today_str}__{safe_query}.json"
    output_path = output_dir / filename

    payload: Dict[str, Any] = {
        "query": query,
        "max_results": max_results,
        "order": order,
        "published_after": published_after,
        "region_code": region_code,
        "fetched_at_utc": today_str,
        "items": detail_items,
        "raw_search_response": search_data,   # í•„ìš”í•˜ë©´ ì¶”í›„ ì œê±° ê°€ëŠ¥
    }

    with output_path.open("w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    logger.info("ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: %s", output_path)
    return output_path


# ------------------------------
# ê°„ë‹¨ ì‹¤í–‰ ì˜ˆì‹œ
# ------------------------------

if __name__ == "__main__":
    test_queries = [
        "Attack on Titan"
    ]

    for q in test_queries:
        try:
            path = search_and_collect(
                query=q,
                max_results=20,
                order="date",
                region_code="KR",  # í•„ìš”ì— ë”°ë¼ ë³€ê²½
            )
            print(f"saved: {path}")
        except Exception as e:
            logger.exception("ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: %s", e)
